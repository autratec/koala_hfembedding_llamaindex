{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Koala7B 8bit + HuggingFaceEmbedding + Llama Index**\n",
        "\n",
        "Code released by: autratec\n",
        "This github version were developed from v2\n",
        "\n",
        "The POC project below enable resercher to setup large language model locally or leverage google colab free environment, wiht vector embedding technology from HaggingFace to do provide an accurate response based on local content through indexing. \n",
        "\n",
        "Colab resouce usage:  RAM: 5.4G. GPU8.9G\n",
        "\n",
        "Pls create a folder callled data and get yoru raw data (csv) in that folder and index.json will be created under root path. Enjoy your test. \n",
        "\n",
        "Putting pipeline outside of class to reduce GPU usage. \n",
        "\n",
        "Here are the reference of codes being used in this notebook: \n",
        "\n",
        "https://colab.research.google.com/drive/10QPfcDt39uGciEDqdYBAbPBNZQDoC99O?usp=sharing\n",
        "\n",
        "https://discord.com/channels/1059199217496772688/1090945925129707570\n",
        "\n"
      ],
      "metadata": {
        "id": "He4AzT9XNeBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VJ7lfTYz0qu"
      },
      "outputs": [],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install datasets loralib sentencepiece \n",
        "!pip -q install bitsandbytes accelerate\n",
        "!pip -q install langchain transformers sentence_transformers llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
        "import torch\n",
        "from llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex, PromptHelper, LLMPredictor, ServiceContext, LangchainEmbedding\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.llms.base import LLM"
      ],
      "metadata": {
        "id": "MK_96JPU0fgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(\"samwit/koala-7b\")\n",
        "model = LlamaForCausalLM.from_pretrained(\"samwit/koala-7b\",load_in_8bit=True,device_map='auto',)\n",
        "pipeline = pipeline(\"text-generation\",model=model, tokenizer=tokenizer, max_length=512,temperature=0.7,top_p=0.95,repetition_penalty=1.15)"
      ],
      "metadata": {
        "id": "qadc-2Th5ejW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customLLM(LLM):\n",
        "    def _call(self, prompt, stop=None):\n",
        "        res = pipeline(prompt)\n",
        "        prompt_length = len(prompt)\n",
        "        return res[0][\"generated_text\"][prompt_length:] \n",
        "    def _identifying_params(self):\n",
        "        return {\"name_of_model\": \"koala-7b\"}\n",
        "    def _llm_type(self):\n",
        "        return \"custom\""
      ],
      "metadata": {
        "id": "XEv_WB2D2UyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple test to ensure LLM is working."
      ],
      "metadata": {
        "id": "5PoWud5RmgsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(customLLM()._call(\"Tell me somthing about New York City.\"))"
      ],
      "metadata": {
        "id": "BnZPAj7NmY5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_size = 512\n",
        "num_output = 200\n",
        "max_chunk_overlap = 20\n",
        "chunk_size_limit = 200\n",
        "\n",
        "llm_predictor = LLMPredictor(llm=customLLM())"
      ],
      "metadata": {
        "id": "4B7xDJrBP-iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())"
      ],
      "metadata": {
        "id": "aXDHnAFaQXOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_helper = PromptHelper(max_input_size, num_output,max_chunk_overlap,chunk_size_limit=chunk_size_limit)\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper, chunk_size_limit = chunk_size_limit) "
      ],
      "metadata": {
        "id": "Eo_y-uNiQDaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a folder called \"data\" and load your csv file for indexing. "
      ],
      "metadata": {
        "id": "QezID0gBo3Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('./data').load_data()\n",
        "index = GPTSimpleVectorIndex.from_documents(documents,service_context=service_context)\n",
        "index.save_to_disk('index.json')"
      ],
      "metadata": {
        "id": "6zloeU6DQFsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = \"My key resouce left the project and it causing the delay. What should i do?\"\n",
        "response = index.query(query_text,response_mode=\"compact\",service_context=service_context, similarity_top_k=1)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "8N6ST3f9nOTF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
